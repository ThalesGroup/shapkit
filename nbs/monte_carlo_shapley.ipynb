{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp monte_carlo_shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Shapley\n",
    "\n",
    "> Estimate the Shapley Values using an optimized Monte Carlo version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley Value definition\n",
    "\n",
    "In Collaborative Game Theory, Shapley Values ([Shapley,1953]) can distribute a reward among players in a fairly way according to their contribution to the win in a cooperative game. We note $\\mathcal{M}$ a set of $d$ players. Moreover, $v : P(\\mathcal{M}) \\rightarrow R_v$ a reward function such that $v(\\emptyset) = 0$. The range $R_v$ can be $\\Re$ or a subset of $\\Re$. $P(\\mathcal{M})$ is a family of sets over $\\mathcal{M}$. If $S \\subset \\mathcal{M}\\text{, } v(S)$ is the amount of wealth produced by coalition $S$ when they cooperate.\n",
    "\n",
    "The Shapley Value of a player $j$ is a fair share of the global wealth $v(\\mathcal{M})$ produced by all players together:\n",
    "\n",
    "$$\\phi_j(\\mathcal{M},v) = \\sum_{S \\subset \\mathcal{M}\\backslash \\{j\\}}\\frac{(d -|S| - 1)!|S|!}{d!}\\left(v(S\\cup \\{j\\}) - v(S)\\right),$$\n",
    "\n",
    "with $|S| = \\text{cardinal}(S)$, i.e. the number of players in coalition $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley Values as contrastive local attribute importance in Machine Learning\n",
    "\n",
    "Let be $X^*\\subset\\Re^d$ a dataset of individuals where a Machine Learning model $f$ is trained and/or tested and $d$  the dimension of $X^*$. $d>1$ else we do not need to compute Shapley Value. We consider the attribute importance of an individual $\\mathbf{x^*} = \\{x_1^*, \\dots, x_d^*\\} \\in X^*$ according to a given reference $\\mathbf{r} = \\{r_1, \\dots, r_d\\}\\in X^*$.  We're looking for $\\boldsymbol{\\phi}=(\\phi_j)_{j\\in\\{1, \\dots, d\\}}\\in \\Re^d$ such that:\n",
    "$$ \\sum_{j=1}^{d} \\phi_j = f(\\mathbf{x^*}) - f(\\mathbf{r}), $$ \n",
    "where $\\phi_j$ is the attribute contribution of feature indexed $j$.  We loosely identify each feature by its column number. Here the set of players $\\mathcal{M}=\\{1, \\dots, d\\}$ is the feature set.\n",
    "\n",
    "In Machine Learning, a common choice for the reward is $ v(S) = \\mathbb{E}[f(X) | X_S = \\mathbf{x_S^*}]$, where $\\mathbf{x_S^*}=(x_j^*)_{j\\in S}$ and $X_S$ the element of $X$ for the coalition $S$. \n",
    "For any $S\\subset\\mathcal{M}$, let's define $ z(\\mathbf{x^*},\\mathbf{r},S)$ such that $z(\\mathbf{x^*},\\mathbf{r},\\emptyset) = \\mathbf{r}$, \\ $z(\\mathbf{x^*},\\mathbf{r},\\mathcal{M}) = \\mathbf{x^*}$ and\n",
    "\n",
    "$$ z(\\mathbf{x^*},\\mathbf{r},S) = (z_1,..., z_d) \\text{ with } z_i =  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "x_i^* & \\mbox{if} \\ i \\in S \\\\\n",
    "r_i & \\mbox{if} \\ i \\notin S\n",
    "\\end{array}\n",
    "\\right. .$$ \n",
    "\n",
    "As explain in [Merrick,2019], each reference $\\textbf{r}$ sets a single-game with $ v(S) = f(z(\\mathbf{x^*},\\mathbf{r},S)) - f(\\mathbf{r}) $, $v(\\emptyset) = 0 $ and $v(\\mathcal{M}) = f(\\mathbf{x^*}) - f(\\mathbf{r}) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Monte Carlo Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs:** instance $\\mathbf{x^*}$ and $\\mathbf{r}$, the reward function $v$ and the number of iterations $\\text{T}$.\n",
    "\n",
    "**Result:** the Shapley Values $\\boldsymbol{\\widehat{\\phi}} \\in \\Re^d$.\n",
    "\n",
    "1.&emsp;Initialization: $\\boldsymbol{\\widehat{\\phi}}=  \\{0,\\dots,0\\} \\text{ and } \\boldsymbol{\\widehat{\\sigma^2}} = \\{0,\\dots,0\\}$ \\;\n",
    "\n",
    "2.&emsp;For $t=1,\\dots,T$:<br>\n",
    "&emsp;&emsp;(a).&emsp;Choose the subset resulting of an uniform permutation $O\\in\\pi(\\{1, \\dots,d\\})$ of the features values \\;<br>\n",
    "&emsp;&emsp;(b).&emsp; $v^{(1)} = v(\\mathbf{r})$, $\\mathbf{b} = \\mathbf{r}$ \\;<br>\n",
    "&emsp;&emsp;(c).&emsp; For j in $O$:<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\\mathbf{b} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "x_i^* & \\mbox{if} \\ i = j \\\\\n",
    "b_i & \\mbox{if} \\ i \\neq j\n",
    "\\end{array}\n",
    "\\right.$, with $i\\in\\{1, \\dots, d\\}$ \\;<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$ v^{(2)} = v(\\mathbf{b})$ \\;<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\\phi_j =  v^{(2)} - v^{(1)}$ \\;<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Update online $\\widehat{\\phi}$ and $\\widehat{\\sigma^2}$ (if $t > 1$):<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\\widehat{\\sigma_j^2} = \\dfrac{t-2}{t-1} \\widehat{\\sigma_j^2}+ (\\phi_j - \\widehat{\\phi_j})^{2/t}$ \\;<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\\widehat{\\phi_j} = \\dfrac{t-1}{t} \\widehat{\\phi_j} + \\dfrac{1}{t} \\phi_j$ \\;<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$v^{(1)} = v^{(2)}$ \\;<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[Shapley,1953] _A value for n-person games_. Lloyd S Shapley. In Contributions to the Theory of Games, 2.28 (1953), pp. 307 - 317.\n",
    "\n",
    "[Merrick,2019] _The Explanation Game: Explaining Machine Learning Models with Cooperative Game Theory_. Luke Merrick, Ankur Taly, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters__\n",
    "\n",
    "* `x`: pandas Series. The instance $\\mathbf{x^*}$ for which we want to calculate Shapley value of each attribute,\n",
    "\n",
    "* `fc`: python function. The reward function $v$,\n",
    "\n",
    "* `r`: pandas Series. The reference $\\mathbf{r}$. The Shapley values (attribute importance) is a contrastive explanation according to this individual,\n",
    "\n",
    "* `n_iter`: integer. The number of iteration, \n",
    "\n",
    "* `callback`: An python object which can be called at each iteration to record distance to minimum for example. At each iteration, callback(Φ)\n",
    "\n",
    "__Returns__\n",
    "\n",
    "* `Φ`: pandas Series. Shapley values of each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MonteCarloShapley(x, fc, r, n_iter, callback=None):\n",
    "    \"\"\"\n",
    "    Estimate the Shapley Values using an optimized Monte Carlo version.\n",
    "    \"\"\"\n",
    "    # Get general information\n",
    "    f_r = fc(r.values)\n",
    "    feature_names = list(x.index)\n",
    "    d = len(feature_names) # dimension\n",
    "\n",
    "    # Store Shapley Values in a pandas Series\n",
    "    # Φ = pd.Series(np.zeros(d), index=feature_names)\n",
    "    Φ_storage = np.empty((n_iter,d))\n",
    "    # Store also the sample variance of the estimator\n",
    "    # σ2 = pd.Series(np.zeros(d), index=feature_names)\n",
    "\n",
    "    # Monte Carlo loop\n",
    "    for m in tqdm(range(1, n_iter+1)):\n",
    "        # Sample a random permutation order\n",
    "        o = np.random.permutation(d)\n",
    "        # init useful variables for this iteration\n",
    "        f_less_j = f_r\n",
    "        x_plus_j = r.values.copy()\n",
    "        for j in o:\n",
    "            x_plus_j[j] = x.values[j]\n",
    "            f_plus_j = fc(x_plus_j)\n",
    "            # update Φ and σ²\n",
    "            Φ_j = f_plus_j - f_less_j\n",
    "            # if m == 1:\n",
    "            #     Φ[name_j] = Φ_j\n",
    "            # else:\n",
    "            #     # σ2[name_j] = (m-2)/(m-1) * σ2[name_j] + (Φ_j - Φ[name_j])**2/m\n",
    "            #     Φ[name_j] = (m-1)/m * Φ[name_j] + 1/m * Φ_j\n",
    "            # Φ[j] += 1/n_iter * Φ_j\n",
    "            Φ_storage[m-1,j] = Φ_j\n",
    "            # reassign f_less_j\n",
    "            f_less_j = f_plus_j\n",
    "        if callback:\n",
    "            Φ = pd.Series(np.mean(Φ_storage[:m,:],axis=0), index=feature_names)\n",
    "            callback(Φ)\n",
    "\n",
    "    Φ = pd.Series(np.mean(Φ_storage,axis=0), index=feature_names)\n",
    "\n",
    "    return Φ\n",
    "    # return Φ #, σ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simulated dataset from the book _Elements of Statistical Learning_ ([hastie,2009], the Radial example). $X_1, \\dots , X_{d}$ are standard independent Gaussian. The model is determined by:\n",
    "\n",
    "$$ Y = \\prod_{j=1}^{d} \\rho(X_j), $$\n",
    "\n",
    "where $\\rho\\text{: } t \\rightarrow \\sqrt{(0.5 \\pi)} \\exp(- t^2 /2)$. The regression function $f_{regr}$ is deterministic and simply defined by $f_r\\text{: } \\textbf{x} \\rightarrow \\prod_{j=1}^{d} \\phi(x_j)$. For a reference $\\mathbf{r^*}$ and a target $\\mathbf{x^*}$, we define the reward function $v_r^{\\mathbf{r^*}, \\mathbf{x^*}}$ such as for each coalition $S$, $v_r^{\\mathbf{r^*}, \\mathbf{x^*}}(S) = f_{regr}(\\mathbf{z}(\\mathbf{x^*}, \\mathbf{r^*}, S)) - f_{regr}(\\mathbf{r^*}).$\n",
    "\n",
    " [hastie,2009] _The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition_. Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome. Springer Series in Statistics, 2009.\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(dim, n_samples, rho=0):\n",
    "    \"\"\"\n",
    "    Generate a dataset of independent Gaussian features\n",
    "    \"\"\"\n",
    "    mu = np.zeros(dim)\n",
    "    sigma = np.ones((dim, dim)) * rho\n",
    "    np.fill_diagonal(sigma, [1] * dim)\n",
    "    # Simulation\n",
    "    X = np.random.multivariate_normal(mean=mu, cov=sigma, size=n_samples)\n",
    "    df_X = pd.DataFrame(X, columns=['x'+str(i) for i in range(1, dim+1)])\n",
    "    return df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, n_samples = 5, 100\n",
    "X = generate_sample(d, n_samples)\n",
    "y = np.zeros(len(X))\n",
    "for i in range(len(X)):\n",
    "    phi_x = np.sqrt(.5 * np.pi) * np.exp(-0.5 * X.values[i] ** 2)\n",
    "    y[i] = np.prod(phi_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension = 5 ; nb of coalitions = 30\n"
     ]
    }
   ],
   "source": [
    "n = 2**d - 2\n",
    "def fc(x):\n",
    "    phi_x = np.sqrt(.5 * np.pi) * np.exp(-0.5 * x ** 2)\n",
    "    return np.prod(phi_x)\n",
    "print(\"dimension = {0} ; nb of coalitions = {1}\".format(str(d), str(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_r, idx_x = np.random.choice(np.arange(len(X)), size=2, replace=False)\n",
    "r = X.iloc[idx_r,:]\n",
    "x = X.iloc[idx_x,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 6739.79it/s]\n"
     ]
    }
   ],
   "source": [
    "mc_shap = MonteCarloShapley(x=x, fc=fc, r=r, n_iter=100, callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1    0.241355\n",
       "x2   -0.399226\n",
       "x3    0.288679\n",
       "x4    0.037454\n",
       "x5    0.079285\n",
       "dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred = fc(r.values)\n",
    "x_pred = fc(x.values)\n",
    "v_M = x_pred - r_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(mc_shap.sum() - v_M) <= 1e-10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted index.ipynb.\n",
      "Converted monte_carlo_shapley.ipynb.\n",
      "Converted shapley_values.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
